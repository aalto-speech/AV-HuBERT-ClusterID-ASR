# toy_model.yaml

## Where the samples will be written
save_data: /scratch/work/sarvasm1/av_hubert/ussee/exp/openNMT_id2char/lstm_0.001lr_10early_stop_1024embed_1024hidden_sent_norm_sep
## Where the vocab(s) will be written
src_vocab: /scratch/work/sarvasm1/av_hubert/ussee/exp/openNMT_id2char/lstm_0.001lr_10early_stop_1024embed_1024hidden_sent_norm_sep/example.vocab.src
tgt_vocab: /scratch/work/sarvasm1/av_hubert/ussee/exp/openNMT_id2char/lstm_0.001lr_10early_stop_1024embed_1024hidden_sent_norm_sep/example.vocab.tgt
# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    corpus_1:
        path_src: /scratch/work/sarvasm1/av_hubert/ussee/dump/avhubert_base_lrs3_iter5_output_head/train_0_1
        path_tgt: /scratch/work/sarvasm1/av_hubert/ussee/dump/train.wrd_tokens_sep
    valid:
        path_src: /scratch/work/sarvasm1/av_hubert/ussee/dump/avhubert_base_lrs3_iter5_output_head/valid_0_1
        path_tgt: /scratch/work/sarvasm1/av_hubert/ussee/dump/valid.wrd_tokens_sep

# Train on a single GPU
world_size: 1
gpu_ranks: [0]
seed: 1234

# Train configs 
optim: adam

batch_size: 512
learning_rate: 0.001
dropout: 0.1
label_smoothing: 0.1

param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'
  #normalization: tokens
early_stopping: 5
early_stopping_criteria: accuracy

# model configs
src_word_vec_size: 1024
tgt_word_vec_size: 1024

enc_hid_size: 1024
dec_hid_size: 1024
model_dtype: fp16

# Where to save the checkpoints
save_model: /scratch/work/sarvasm1/av_hubert/ussee/exp/openNMT_id2char/lstm_0.001lr_10early_stop_1024embed_1024hidden_sent_norm_sep/checkpoints/model
save_checkpoint_steps: 500
train_steps: 100000
keep_checkpoint: 10
valid_steps: 500
